{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      AT      V       AP     RH      PE\n",
      "0  14.96  41.76  1024.07  73.17  463.26\n",
      "1  25.18  62.96  1020.04  59.08  444.37\n",
      "2   5.11  39.40  1012.16  92.14  488.56\n",
      "3  20.86  57.32  1010.24  76.64  446.48\n",
      "4  10.82  37.50  1009.23  96.62  473.90\n"
     ]
    }
   ],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset = pd.read_excel('./Dataset/CCPP/CCPP_dataset.xlsx')\n",
    "\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.62951938 -0.98729659  1.8204884  -0.00951935  0.52120807]\n",
      " [ 0.74190911  0.68104512  1.1418628  -0.97462052 -0.58566442]\n",
      " [-1.95129733 -1.17301765 -0.18507756  1.2898397   2.00367889]\n",
      " ...\n",
      " [ 1.56583973  1.57581141 -0.05709854 -2.52261812 -1.45288056]\n",
      " [ 0.64797565  1.19177803  0.1011913  -0.74790051 -1.09134518]\n",
      " [ 0.26150656  0.64641916  0.66867722 -0.37254534 -0.06357687]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit_transform(dataset))\n",
    "Dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = Dataset[:, : 4] , Dataset[: , 4]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "#print(X_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "svm_reg =    SVR()\n",
    "sgd_reg =    linear_model.SGDRegressor()\n",
    "bay_reg =    linear_model.BayesianRidge()\n",
    "las_reg =    linear_model.LassoLars()\n",
    "ard_reg =    linear_model.ARDRegression()\n",
    "pag_reg =    linear_model.PassiveAggressiveRegressor()\n",
    "lin_reg =    linear_model.LinearRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR        0.05294232716855608\n",
      "SGDRegressor        0.06950281460887694\n",
      "BayesianRidge        0.06960745120298706\n",
      "LassoLars        0.9962749092436307\n",
      "ARDRegression        0.06960922620737599\n",
      "PassiveAggressiveRegressor        0.13699492411433847\n",
      "LinearRegression        0.06960884287546201\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "for reg in (svm_reg, sgd_reg, bay_reg, las_reg, ard_reg, pag_reg, lin_reg):\n",
    "    reg.fit(X_train, y_train)\n",
    "    prediction = reg.predict(X_test)\n",
    "    print(reg.__class__.__name__, \"      \", mean_squared_error(y_test, prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot use ensemble learning on countinous output. Therefore wwe are converting our countinous output( Energy Produced ) into categorial values (Classes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import hist\n",
    "\n",
    "dataset[\"PE\"] = np.ceil(dataset[\"PE\"] / 8)\n",
    "dataset[\"PE\"].where(dataset[\"PE\"] > 54 , 54 , inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.0    1775\n",
       "56.0    1745\n",
       "59.0    1273\n",
       "57.0    1198\n",
       "60.0    1050\n",
       "58.0    1030\n",
       "54.0     677\n",
       "61.0     652\n",
       "62.0     168\n",
       "Name: PE, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"PE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x23279a574e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAFuZJREFUeJzt3X2MXXWdx/H3R7oQllEetnC3tmSLSWEXmNils9WNQWcWFwoqDxrcEgIFNAUDRpMaLetmJRIiPnTNAi7uKBVYkZHlwXahKNUwEBOKUCxMeQoDjDIt266C1REWM/jdP+5vlmudmTv3nDv33vb3eSWTe+7v/M453/sw87nnd865o4jAzMzy9KZ2F2BmZu3jEDAzy5hDwMwsYw4BM7OMOQTMzDLmEDAzy5hDwMwsYw4BM7OMOQTMzDI2p90F1DN37txYuHBhoWV/+9vfcsABBzS3oCZwXY1xXY1xXY3ZG+vavHnzLyLi0Bl1joiO/lmyZEkUde+99xZedja5rsa4rsa4rsbsjXUBD8cM/8Z6OMjMLGMOATOzjDkEzMwy5hAwM8uYQ8DMLGMOATOzjDkEzMwy5hAwM8tY3RCQtFbSTklba9q+K2lL+hmRtCW1L5T0as28r9css0TSkKRhSVdJ0uw8JDMzm6mZfG3E9cA1wI0TDRHxDxPTktYAu2r6PxsRiydZz7XASmATsAFYBtzdeMmdb+Hqu6adv6p7nPPq9Clq5Mr3zcp6zWzvVHdPICLuB16abF76NP9h4Obp1iFpHvCWiHggXdJ8I3B64+WamVkzlT0mcDywIyKeqWk7QtJPJd0n6fjUNh8YrekzmtrMzKyNVP1gXqeTtBC4MyKO3a39WmA4Itak+/sBXRHxS0lLgO8BxwBHAV+IiPemfscDn46ID0yxvZVUh46oVCpLBgYGCj24sbExurq6Ci1bxtC2XdPOr+wPO15tUTENKFNX9/wDm1tMjXa9jvW4rsa4rsaUqauvr29zRPTMpG/hr5KWNAf4ILBkoi0iXgNeS9ObJT0LHEn1k/+CmsUXANunWndE9AP9AD09PdHb21uoxsHBQYouW0a98f5V3eOsGeq8b/EuU9fI2b3NLaZGu17HelxXY1xXY1pVV5nhoPcCT0XE/w/zSDpU0j5p+m3AIuC5iHgR+I2kd6bjCOcC60ps28zMmmAmp4jeDDwAHCVpVNJH0qzl/PEB4XcDj0l6FLgVuCgiJg4qfwz4JjAMPMteemaQmdmepO6+f0ScNUX7eZO03QbcNkX/h4FjJ5tnZmbt4SuGzcwy5hAwM8uYQ8DMLGMOATOzjDkEzMwy5hAwM8uYQ8DMLGMOATOzjDkEzMwy5hAwM8uYQ8DMLGMOATOzjDkEzMwy5hAwM8uYQ8DMLGMOATOzjDkEzMwy5hAwM8uYQ8DMLGMOATOzjNUNAUlrJe2UtLWm7TJJ2yRtST+n1My7VNKwpKclnVTTviy1DUta3fyHYmZmjZrJnsD1wLJJ2r8aEYvTzwYASUcDy4Fj0jL/JmkfSfsAXwNOBo4Gzkp9zcysjebU6xAR90taOMP1nQYMRMRrwPOShoGlad5wRDwHIGkg9X2i4YrNzKxpyhwTuETSY2m46ODUNh94oabPaGqbqt3MzNpIEVG/U3VP4M6IODbdrwC/AAK4HJgXERdI+hrwQER8O/W7DthANWxOioiPpvZzgKUR8fEptrcSWAlQqVSWDAwMFHpwY2NjdHV1FVq2jKFtu6adX9kfdrzaomIaUKau7vkHNreYGu16HetxXY1xXY0pU1dfX9/miOiZSd+6w0GTiYgdE9OSvgHcme6OAofXdF0AbE/TU7VPtv5+oB+gp6cnent7i5TJ4OAgRZct47zVd007f1X3OGuGCj31s6pMXSNn9za3mBrteh3rcV2NcV2NaVVdhYaDJM2ruXsGMHHm0HpguaT9JB0BLAJ+AjwELJJ0hKR9qR48Xl+8bDMza4a6H/sk3Qz0AnMljQKfA3olLaY6HDQCXAgQEY9LuoXqAd9x4OKIeD2t5xLgB8A+wNqIeLzpj8bMzBoyk7ODzpqk+bpp+l8BXDFJ+waqxwfMzKxD+IphM7OMOQTMzDLmEDAzy5hDwMwsYw4BM7OMOQTMzDLmEDAzy5hDwMwsYw4BM7OMOQTMzDLmEDAzy5hDwMwsYw4BM7OMOQTMzDLmEDAzy5hDwMwsYw4BM7OMOQTMzDLmEDAzy5hDwMwsY3VDQNJaSTslba1p+7KkpyQ9JukOSQel9oWSXpW0Jf18vWaZJZKGJA1LukqSZuchmZnZTM2ZQZ/rgWuAG2vaNgKXRsS4pC8ClwKfSfOejYjFk6znWmAlsAnYACwD7i5Yt1m2Fq6+a9r5q7rHOa9On6JGrnzfrKzX2qfunkBE3A+8tFvbPRExnu5uAhZMtw5J84C3RMQDERFUA+X0YiWbmVmzNOOYwAX84Sf6IyT9VNJ9ko5PbfOB0Zo+o6nNzMzaSNUP5nU6SQuBOyPi2N3aPwv0AB+MiJC0H9AVEb+UtAT4HnAMcBTwhYh4b1rueODTEfGBKba3kurQEZVKZcnAwEChBzc2NkZXV1ehZcsY2rZr2vmV/WHHqy0qpgFl6uqef2Bzi6nRrtexnhzfX2VeZ7+OjSlTV19f3+aI6JlJ35kcE5iUpBXA+4ET0hAPEfEa8Fqa3izpWeBIqp/8a4eMFgDbp1p3RPQD/QA9PT3R29tbqMbBwUGKLltGvfHYVd3jrBkq/NTPmjJ1jZzd29xiarTrdawnx/dXmdfZr2NjWlVXoeEgScuoHgg+NSJeqWk/VNI+afptwCLguYh4EfiNpHems4LOBdaVrt7MzEqp+3FB0s1ALzBX0ijwOapnA+0HbExnem6KiIuAdwOflzQOvA5cFBETB5U/RvVMo/2pHkPwmUFmZm1WNwQi4qxJmq+bou9twG1TzHsYOHayeWZm1h6+YtjMLGOdd3TS9kj1LmAqo97FT76Ayaw47wmYmWXMIWBmljGHgJlZxhwCZmYZcwiYmWXMIWBmljGHgJlZxhwCZmYZcwiYmWXMIWBmljGHgJlZxhwCZmYZcwiYmWXMIWBmljGHgJlZxhwCZmYZ8z+VMbMZK/PPg+r9c6Dp+B8HzR7vCZiZZWxGISBpraSdkrbWtB0iaaOkZ9Ltwaldkq6SNCzpMUnH1SyzIvV/RtKK5j8cMzNrxEyHg64HrgFurGlbDfwoIq6UtDrd/wxwMrAo/bwDuBZ4h6RDgM8BPUAAmyWtj4iXm/FAzFptaNuuwsMbZp1iRnsCEXE/8NJuzacBN6TpG4DTa9pvjKpNwEGS5gEnARsj4qX0h38jsKzsAzAzs+LKHBOoRMSLAOn2sNQ+H3ihpt9oapuq3czM2mQ2zg7SJG0xTfsfr0BaCawEqFQqDA4OFipkbGys8LJlrOoen3Z+Zf/6fdphT62rHa8x7LnPV7uUqWs2X+N2/Z2op1V1lQmBHZLmRcSLabhnZ2ofBQ6v6bcA2J7ae3drH5xsxRHRD/QD9PT0RG9v72Td6hocHKTosmXUGyde1T3OmqHOOzt3T61r5Oze1hVT4+qb1u2Rz1e7lKlrNl/jdv2dqKdVdZUZDloPTJzhswJYV9N+bjpL6J3ArjRc9APgREkHpzOJTkxtZmbWJjOKZUk3U/0UP1fSKNWzfK4EbpH0EeDnwJmp+wbgFGAYeAU4HyAiXpJ0OfBQ6vf5iNj9YLOZmbXQjEIgIs6aYtYJk/QN4OIp1rMWWDvj6szMbFb5imEzs4w5BMzMMuYQMDPLmEPAzCxjDgEzs4w5BMzMMuYQMDPLmEPAzCxjDgEzs4w5BMzMMuYQMDPLmEPAzCxjDgEzs4w5BMzMMuYQMDPLmEPAzCxjDgEzs4w5BMzMMuYQMDPLmEPAzCxjDgEzs4wVDgFJR0naUvPza0mflHSZpG017afULHOppGFJT0s6qTkPwczMippTdMGIeBpYDCBpH2AbcAdwPvDViPhKbX9JRwPLgWOAtwI/lHRkRLxetAYzMyunWcNBJwDPRsTPpulzGjAQEa9FxPPAMLC0Sds3M7MCFBHlVyKtBR6JiGskXQacB/waeBhYFREvS7oG2BQR307LXAfcHRG3TrK+lcBKgEqlsmRgYKBQXWNjY3R1dRVatoyhbbumnV/ZH3a82qJiGrCn1tU9/8DWFVNj50u79sjnq13K1DWbr3G7/k7UU6auvr6+zRHRM5O+hYeDJkjaFzgVuDQ1XQtcDkS6XQNcAGiSxSdNoIjoB/oBenp6ore3t1Btg4ODFF22jPNW3zXt/FXd46wZKv3UN92eWtfI2b2tK6bG1Tet2yOfr3YpU9dsvsbt+jtRT6vqasZw0MlU9wJ2AETEjoh4PSJ+D3yDN4Z8RoHDa5ZbAGxvwvbNzKygZoTAWcDNE3ckzauZdwawNU2vB5ZL2k/SEcAi4CdN2L6ZmRVUap9R0p8Cfw9cWNP8JUmLqQ71jEzMi4jHJd0CPAGMAxf7zCAzs/YqFQIR8QrwZ7u1nTNN/yuAK8ps08zMmsdXDJuZZcwhYGaWMYeAmVnGOu9k4iYa2rar7jn7ZmY5856AmVnGHAJmZhnbq4eDzGzvsHAWh3VXdY9PO2w8cuX7Zm3bncB7AmZmGXMImJllzCFgZpYxh4CZWcYcAmZmGXMImJllzCFgZpYxh4CZWcYcAmZmGXMImJllzCFgZpYxh4CZWcYcAmZmGSsdApJGJA1J2iLp4dR2iKSNkp5Jtwendkm6StKwpMckHVd2+2ZmVlyz9gT6ImJxRPSk+6uBH0XEIuBH6T7AycCi9LMSuLZJ2zczswJmazjoNOCGNH0DcHpN+41RtQk4SNK8WarBzMzqUESUW4H0PPAyEMC/R0S/pF9FxEE1fV6OiIMl3QlcGRE/Tu0/Aj4TEQ/vts6VVPcUqFQqSwYGBgrVtvOlXex4tdCis6qyP66rAfXq6p5/YOuKqeH3V2P21Lra9f4aGxujq6ur0LJ9fX2ba0ZmptWM/yz2rojYLukwYKOkp6bpq0na/iiFIqIf6Afo6emJ3t7eQoVdfdM61gx13j9PW9U97roaUK+ukbN7W1dMDb+/GrOn1tWu99fg4CBF//Y1ovRwUERsT7c7gTuApcCOiWGedLszdR8FDq9ZfAGwvWwNZmZWTKkQkHSApDdPTAMnAluB9cCK1G0FsC5NrwfOTWcJvRPYFREvlqnBzMyKK7tvVgHukDSxru9ExPclPQTcIukjwM+BM1P/DcApwDDwCnB+ye2bmVkJpUIgIp4D3j5J+y+BEyZpD+DiMts0M7Pm8RXDZmYZcwiYmWXMIWBmljGHgJlZxhwCZmYZcwiYmWXMIWBmljGHgJlZxhwCZmYZcwiYmWXMIWBmljGHgJlZxhwCZmYZcwiYmWXMIWBmljGHgJlZxhwCZmYZcwiYmWXMIWBmljGHgJlZxgqHgKTDJd0r6UlJj0v6RGq/TNI2SVvSzyk1y1wqaVjS05JOasYDMDOz4uaUWHYcWBURj0h6M7BZ0sY076sR8ZXazpKOBpYDxwBvBX4o6ciIeL1EDWZmVkLhPYGIeDEiHknTvwGeBOZPs8hpwEBEvBYRzwPDwNKi2zczs/KackxA0kLgr4EHU9Mlkh6TtFbSwaltPvBCzWKjTB8aZmY2yxQR5VYgdQH3AVdExO2SKsAvgAAuB+ZFxAWSvgY8EBHfTstdB2yIiNsmWedKYCVApVJZMjAwUKi2nS/tYserhRadVZX9cV0NqFdX9/wDW1dMDb+/GrOn1tWu99fY2BhdXV2Flu3r69scET0z6VvmmACS/gS4DbgpIm4HiIgdNfO/AdyZ7o4Ch9csvgDYPtl6I6If6Afo6emJ3t7eQvVdfdM61gyVeoizYlX3uOtqQL26Rs7ubV0xNfz+asyeWle73l+Dg4MU/dvXiDJnBwm4DngyIv6lpn1eTbczgK1pej2wXNJ+ko4AFgE/Kbp9MzMrr0wsvws4BxiStCW1/SNwlqTFVIeDRoALASLicUm3AE9QPbPoYp8ZZGbWXoVDICJ+DGiSWRumWeYK4Iqi2zQzs+byFcNmZhlzCJiZZcwhYGaWMYeAmVnGOu+kXTOzDrJw9V1t2e71yw5oyXa8J2BmljGHgJlZxhwCZmYZcwiYmWXMIWBmljGHgJlZxhwCZmYZcwiYmWXMIWBmljGHgJlZxhwCZmYZcwiYmWXMIWBmljGHgJlZxhwCZmYZa3kISFom6WlJw5JWt3r7Zmb2hpaGgKR9gK8BJwNHA2dJOrqVNZiZ2RtavSewFBiOiOci4nfAAHBai2swM7Ok1SEwH3ih5v5oajMzszZQRLRuY9KZwEkR8dF0/xxgaUR8fLd+K4GV6e5RwNMFNzkX+EXBZWeT62qM62qM62rM3ljXX0TEoTPp2Op/ND8KHF5zfwGwffdOEdEP9JfdmKSHI6Kn7HqazXU1xnU1xnU1Jve6Wj0c9BCwSNIRkvYFlgPrW1yDmZklLd0TiIhxSZcAPwD2AdZGxOOtrMHMzN7Q6uEgImIDsKFFmys9pDRLXFdjXFdjXFdjsq6rpQeGzcyss/hrI8zMMrbXhICkEUlDkrZIeni3eZ+SFJLmdkJdki6TtC21bZF0SifUldo/nr7W43FJX+qEuiR9t+a5GpG0pUPqWixp00SbpKUdUtfbJT2Q2v9L0ltaXVeq4yBJt0p6StKTkv5W0iGSNkp6Jt0e3AE1nZne77+X1LazhKao7cvp/mOS7pB0UNM3HBF7xQ8wAsydpP1wqgeifzbZ/HbUBVwGfKrTni+gD/ghsF+6f1gn1LXb/DXAP3dCXcA9wMlp+hRgsEPqegh4T5q+ALi81XWlbd8AfDRN7wscBHwJWJ3aVgNf7ICa/orq9UiDQE87nqtpajsRmJPavjgbz9desycwja8CnwZ88KO+jwFXRsRrABGxs831/AFJAj4M3NzuWpIAJj5lH8gk17y0yVHA/Wl6I/ChVheQ9j7eDVwHEBG/i4hfUf2amBtStxuA09tdU0Q8GRFFL0id7druiYjx1G0T1WurmmpvCoEA7pG0OV1xjKRTgW0R8Wgn1ZVcknbx1rZ6l3iauo4Ejpf0oKT7JP1Nh9Q14XhgR0Q80yF1fRL4sqQXgK8Al3ZIXVuBU9P0mfzhBZqt8jbgf4BvSfqppG9KOgCoRMSLAOn2sA6oqRPMpLYLgLubvuV27frMwq7UW9PtYcCjVFP1QeDA1D5Ce4aDJqurQvU6iTcBV1C9XqIT6toKXAWI6pf9PU86g6ydddXMuxZY1UHvr6uAD6X2DwM/7JC6/pLqUNVm4HPAL9tQVw8wDrwj3f9X4HLgV7v1e7ndNdXMH6RNw0EzqO2zwB2z8fu41+wJRMT2dLuT6pP1HuAI4FFJI1R3ox6R9OdtrmtpROyIiNcj4vfAN6j+wW2pyeqi+rUet0fVT4DfU/3+knbXhaQ5wAeB77aynjp1rQBuT13+kw55HSPiqYg4MSKWUB06e7bVdVF9L41GxIPp/q3AccAOSfMA0m0rhxynqqkTTFmbpBXA+4GzIyVCM+0VISDpAElvnpimejDloYg4LCIWRsRCqk/ycRHx322ua+vEL0FyBtVP4C0zVV3A94C/S+1HUj041bIv1pqmLoD3Ak9FxGir6plBXdupftiA6vPW0mGqad5fh6W2NwH/BHy9lXUBpN+zFyQdlZpOAJ6g+jUxK1LbCmBdB9TUdlPVJmkZ8Bng1Ih4ZTa23fIrhmdJBbijetyQOcB3IuL77S0JmKIuSf8haTHV8dwR4MIOqWtfYK2krcDvgBWz8cmj0brSvOW074DwVM/XGPCvaS/lf3njm2/bXdcnJF2c+twOfKvFdU34OHBTel89B5xP9YPnLZI+Avyc6jGLttYk6QzgauBQ4C5JWyLipBbXNWltVM/02g/YmF7nTRFxUTM36iuGzcwytlcMB5mZWTEOATOzjDkEzMwy5hAwM8uYQ8DMLGMOATOzjDkEzMwy5hAwM8vY/wHRWzFRiU1jowAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[\"PE\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_train          AT      V       AP     RH    PE\n",
      "5487  21.92  49.02  1009.29  88.56  56.0\n",
      "3522  11.09  40.43  1025.47  74.97  62.0\n",
      "6916   8.49  39.61  1021.05  87.74  61.0\n",
      "7544  11.43  44.78  1013.43  82.45  59.0\n",
      "7600  17.28  39.99  1007.09  74.25  58.0\n",
      "4374  32.00  73.17  1009.92  42.47  55.0\n",
      "7502  30.75  71.25  1000.07  65.47  54.0\n",
      "2982  26.25  66.56  1008.29  60.65  55.0\n",
      "1210  26.82  69.05  1004.62  40.00  55.0\n",
      "564   18.59  43.14  1011.92  52.63  59.0\n",
      "3680  23.03  65.12  1016.29  69.80  56.0\n",
      "2948  23.25  70.32  1009.28  86.52  55.0\n",
      "6666  10.33  40.62  1017.41  64.22  60.0\n",
      "5253  24.45  66.49  1014.24  58.15  57.0\n",
      "4392  25.72  66.86  1007.47  69.14  55.0\n",
      "2045  18.82  61.27  1019.50  72.23  57.0\n",
      "5138  21.28  70.72  1010.29  86.22  56.0\n",
      "6829  12.02  41.92  1030.10  84.45  59.0\n",
      "5030  20.92  49.15  1021.15  49.76  57.0\n",
      "2648  15.75  36.99  1007.67  93.80  59.0\n",
      "4119  22.89  62.39  1007.84  88.16  57.0\n",
      "5875  25.74  63.73  1010.88  70.87  56.0\n",
      "9036  25.33  71.73  1009.78  77.78  55.0\n",
      "742   15.26  46.18  1013.68  98.58  58.0\n",
      "180   25.42  59.04  1011.98  68.78  56.0\n",
      "1115   9.46  42.28  1008.67  78.16  61.0\n",
      "4420  20.24  58.41  1013.73  99.59  57.0\n",
      "3091  17.92  44.20  1018.97  53.80  57.0\n",
      "2157  23.96  51.30  1011.81  71.81  57.0\n",
      "5173  10.32  41.01  1021.04  95.30  59.0\n",
      "...     ...    ...      ...    ...   ...\n",
      "2734  13.58  40.75  1016.05  76.45  60.0\n",
      "189   17.34  44.78  1007.81  56.38  57.0\n",
      "9167  28.80  71.77  1006.41  66.03  54.0\n",
      "2747  22.40  70.94  1008.06  84.52  56.0\n",
      "2047   7.00  41.55  1001.43  97.12  61.0\n",
      "7849  20.11  42.04  1010.79  60.31  59.0\n",
      "2558  13.74  45.08  1024.97  84.79  59.0\n",
      "9274  22.95  45.61  1013.65  73.92  58.0\n",
      "8666  27.00  69.40  1004.86  82.20  55.0\n",
      "6396  21.46  50.59  1016.75  67.14  57.0\n",
      "3385  11.92  41.20  1016.99  81.24  60.0\n",
      "4555  29.61  67.07  1005.91  39.28  55.0\n",
      "1184  14.62  46.18  1014.75  96.81  58.0\n",
      "6420   9.27  38.56  1017.69  62.49  60.0\n",
      "5051  25.35  72.99  1007.65  84.09  55.0\n",
      "5311  28.50  71.32  1002.54  69.35  55.0\n",
      "2433  24.24  70.32  1009.38  82.83  55.0\n",
      "6949  10.11  39.35  1015.19  90.74  60.0\n",
      "769    9.06  39.30  1019.73  84.23  61.0\n",
      "1685  32.56  68.94  1007.12  58.18  54.0\n",
      "8322  15.47  44.90  1021.59  81.74  59.0\n",
      "5578  17.08  40.12  1012.17  81.50  58.0\n",
      "4426  18.91  59.21  1017.76  90.49  57.0\n",
      "466   19.29  50.16  1010.49  97.78  57.0\n",
      "6265  23.50  50.78  1008.70  57.70  56.0\n",
      "5734  23.39  61.87  1010.35  48.91  56.0\n",
      "5191  11.63  40.55  1022.89  87.12  60.0\n",
      "5390  27.79  69.23  1012.86  45.71  55.0\n",
      "860   12.26  41.50  1014.87  89.41  59.0\n",
      "7270  11.75  40.55  1018.09  97.70  60.0\n",
      "\n",
      "[7654 rows x 5 columns]\n",
      "A_test          AT      V       AP     RH    PE\n",
      "2513  19.64  48.06  1014.81  74.96  57.0\n",
      "9411  28.26  69.23  1013.01  42.10  55.0\n",
      "8745  27.98  67.17  1007.32  75.29  56.0\n",
      "9085  28.64  69.23  1013.11  37.13  55.0\n",
      "4950   9.34  38.08  1019.56  67.74  61.0\n",
      "2755  25.50  68.08  1011.13  72.86  55.0\n",
      "563   21.96  59.80  1016.72  72.60  57.0\n",
      "5834  27.17  73.50  1010.36  73.29  55.0\n",
      "6850  25.96  71.64  1003.84  86.83  55.0\n",
      "4359  10.38  40.92  1021.53  95.18  59.0\n",
      "5215  18.68  59.14  1016.96  77.09  57.0\n",
      "6039  21.16  78.11  1006.73  78.11  55.0\n",
      "4738  12.86  40.60  1013.60  84.85  60.0\n",
      "2420  28.92  66.51  1015.50  34.51  55.0\n",
      "7999  19.14  56.65  1020.84  82.97  58.0\n",
      "8788  14.02  44.47  1028.02  68.97  59.0\n",
      "7169   7.67  42.85  1012.02  93.35  60.0\n",
      "1617  21.11  65.94  1009.83  71.91  57.0\n",
      "1688  14.33  38.73  1003.52  85.92  59.0\n",
      "1488  28.33  68.27  1008.09  50.37  55.0\n",
      "1418   9.63  41.14  1027.99  87.89  59.0\n",
      "6796  32.84  77.95  1014.68  45.80  54.0\n",
      "4226  32.96  69.75  1008.86  40.23  55.0\n",
      "4828  17.48  40.89  1011.39  62.45  58.0\n",
      "5883  19.39  51.43  1011.06  94.35  58.0\n",
      "7471  13.62  42.34  1017.87  89.67  59.0\n",
      "2865  26.35  59.44  1012.16  63.73  56.0\n",
      "1263  27.34  71.64  1008.12  72.22  55.0\n",
      "5013  14.30  53.82  1016.64  63.90  59.0\n",
      "5873  14.72  41.23   997.91  82.69  59.0\n",
      "...     ...    ...      ...    ...   ...\n",
      "5434  12.47  45.01  1017.80  95.25  59.0\n",
      "2651  22.80  73.17  1011.87  91.57  55.0\n",
      "6583  25.76  73.50  1011.44  81.36  55.0\n",
      "2683  14.85  45.87  1009.37  84.65  58.0\n",
      "8939  27.60  67.25  1017.78  54.25  55.0\n",
      "7650  32.87  70.80  1009.54  59.85  55.0\n",
      "6432   4.47  35.19  1018.78  92.68  61.0\n",
      "4919  12.78  41.04  1025.16  85.14  58.0\n",
      "1226  12.11  38.28  1013.56  93.21  59.0\n",
      "9465  29.65  72.86  1004.26  69.63  55.0\n",
      "5605  24.57  66.54  1004.44  83.15  55.0\n",
      "96    10.08  40.72  1022.70  67.30  61.0\n",
      "4665  22.04  59.14  1017.34  84.75  56.0\n",
      "3031  23.13  61.47  1008.96  85.50  55.0\n",
      "6322  20.76  69.05  1001.89  77.86  56.0\n",
      "6443  22.11  59.44  1012.82  84.67  57.0\n",
      "5314   9.59  34.69  1027.65  75.32  60.0\n",
      "8462   8.96  40.02  1031.21  82.32  60.0\n",
      "3126  13.92  43.41  1015.12  61.64  59.0\n",
      "4438  25.01  59.54  1007.91  80.93  56.0\n",
      "8807  22.92  58.62  1016.37  71.38  56.0\n",
      "2833  30.57  69.71  1008.94  59.99  55.0\n",
      "1017   6.90  39.81  1016.74  88.24  61.0\n",
      "6409  26.30  59.07  1007.84  78.32  56.0\n",
      "2650  20.12  69.45  1013.23  88.48  56.0\n",
      "7204  29.06  64.96  1000.88  62.07  54.0\n",
      "1599   9.87  40.81  1017.17  84.25  60.0\n",
      "5697   8.02  39.04  1018.49  68.07  60.0\n",
      "350   26.48  69.14  1009.31  84.11  55.0\n",
      "6210  15.34  71.14  1019.79  77.56  58.0\n",
      "\n",
      "[1914 rows x 5 columns]\n",
      "b_train 5487    56.0\n",
      "3522    62.0\n",
      "6916    61.0\n",
      "7544    59.0\n",
      "7600    58.0\n",
      "4374    55.0\n",
      "7502    54.0\n",
      "2982    55.0\n",
      "1210    55.0\n",
      "564     59.0\n",
      "3680    56.0\n",
      "2948    55.0\n",
      "6666    60.0\n",
      "5253    57.0\n",
      "4392    55.0\n",
      "2045    57.0\n",
      "5138    56.0\n",
      "6829    59.0\n",
      "5030    57.0\n",
      "2648    59.0\n",
      "4119    57.0\n",
      "5875    56.0\n",
      "9036    55.0\n",
      "742     58.0\n",
      "180     56.0\n",
      "1115    61.0\n",
      "4420    57.0\n",
      "3091    57.0\n",
      "2157    57.0\n",
      "5173    59.0\n",
      "        ... \n",
      "2734    60.0\n",
      "189     57.0\n",
      "9167    54.0\n",
      "2747    56.0\n",
      "2047    61.0\n",
      "7849    59.0\n",
      "2558    59.0\n",
      "9274    58.0\n",
      "8666    55.0\n",
      "6396    57.0\n",
      "3385    60.0\n",
      "4555    55.0\n",
      "1184    58.0\n",
      "6420    60.0\n",
      "5051    55.0\n",
      "5311    55.0\n",
      "2433    55.0\n",
      "6949    60.0\n",
      "769     61.0\n",
      "1685    54.0\n",
      "8322    59.0\n",
      "5578    58.0\n",
      "4426    57.0\n",
      "466     57.0\n",
      "6265    56.0\n",
      "5734    56.0\n",
      "5191    60.0\n",
      "5390    55.0\n",
      "860     59.0\n",
      "7270    60.0\n",
      "Name: PE, Length: 7654, dtype: float64\n",
      "b_test 2513    57.0\n",
      "9411    55.0\n",
      "8745    56.0\n",
      "9085    55.0\n",
      "4950    61.0\n",
      "2755    55.0\n",
      "563     57.0\n",
      "5834    55.0\n",
      "6850    55.0\n",
      "4359    59.0\n",
      "5215    57.0\n",
      "6039    55.0\n",
      "4738    60.0\n",
      "2420    55.0\n",
      "7999    58.0\n",
      "8788    59.0\n",
      "7169    60.0\n",
      "1617    57.0\n",
      "1688    59.0\n",
      "1488    55.0\n",
      "1418    59.0\n",
      "6796    54.0\n",
      "4226    55.0\n",
      "4828    58.0\n",
      "5883    58.0\n",
      "7471    59.0\n",
      "2865    56.0\n",
      "1263    55.0\n",
      "5013    59.0\n",
      "5873    59.0\n",
      "        ... \n",
      "5434    59.0\n",
      "2651    55.0\n",
      "6583    55.0\n",
      "2683    58.0\n",
      "8939    55.0\n",
      "7650    55.0\n",
      "6432    61.0\n",
      "4919    58.0\n",
      "1226    59.0\n",
      "9465    55.0\n",
      "5605    55.0\n",
      "96      61.0\n",
      "4665    56.0\n",
      "3031    55.0\n",
      "6322    56.0\n",
      "6443    57.0\n",
      "5314    60.0\n",
      "8462    60.0\n",
      "3126    59.0\n",
      "4438    56.0\n",
      "8807    56.0\n",
      "2833    55.0\n",
      "1017    61.0\n",
      "6409    56.0\n",
      "2650    56.0\n",
      "7204    54.0\n",
      "1599    60.0\n",
      "5697    60.0\n",
      "350     55.0\n",
      "6210    58.0\n",
      "Name: PE, Length: 1914, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "A, b = dataset.loc[:, : \"PE\"] , dataset.loc[: , \"PE\"]\n",
    "\n",
    "A_train, A_test, b_train, b_test = train_test_split(A, b, test_size = 0.2, random_state = 42)\n",
    "print(\"A_train\", A_train)\n",
    "print(\"A_test\", A_test)\n",
    "print(\"b_train\", b_train)\n",
    "print(\"b_test\", b_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier( estimators = [('lr', log_reg), ('rf', rnd_clf), ('svc', svm_clf)], voting = 'hard')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58. 55. 55. ... 60. 55. 56.]\n",
      "LogisticRegression 0.6342737722048067\n",
      "[57. 55. 56. ... 60. 55. 58.]\n",
      "RandomForestClassifier 0.9989550679205852\n",
      "[57. 55. 55. ... 61. 55. 58.]\n",
      "SVC 0.8087774294670846\n",
      "[57. 55. 55. ... 60. 55. 58.]\n",
      "VotingClassifier 0.9054336468129571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_reg, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(A_train, b_train)\n",
    "    y_pred = clf.predict(A_test)\n",
    "    print(y_pred)\n",
    "    print(clf.__class__.__name__, accuracy_score(b_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier is overfitting the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy score of 1.0 mean that the model is overfitting the data.\n",
    "To check this lets use Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.         0.         0.03610791 0.03610791 0.03610791 0.\n",
      " 0.         0.05123155 0.         0.05129892]\n",
      "Mean: 0.02108541912632315\n",
      "Standard Deviation: 0.021729345964491244\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(rnd_clf, A_train , b_train, scoring = \"neg_mean_squared_error\", cv = 10)\n",
    "rmse_scores = np.sqrt(-scores) \n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard Deviation:\", scores.std())\n",
    "    \n",
    "display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our model was overfitting the data. Lets tweak parameters of random forest classifier to correct this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9696969696969697\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "ran_clf = RandomForestClassifier(n_estimators = 50, max_leaf_nodes = 8, n_jobs = -1)\n",
    "ran_clf.fit(A_train, b_train)\n",
    "\n",
    "b_pred = ran_clf.predict(A_test)\n",
    "print(accuracy_score(b_test, b_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9869383490073145\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "bag_clf = BaggingClassifier( DecisionTreeClassifier(), n_estimators = 10, max_samples = 50, bootstrap = True, n_jobs = -1)\n",
    "bag_clf.fit(A_train, b_train)\n",
    "\n",
    "y_pred = bag_clf.predict(A_test)\n",
    "print(accuracy_score(b_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging classifier improved accuracy over the random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.         0.12508146 0.16546722 0.13510342 0.09553254 0.13018891\n",
      " 0.13027386 0.10867853 0.05123155 0.13078709]\n",
      "Mean: 0.1072344588832836\n",
      "Standard Deviation: 0.04568727502365886\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(bag_clf, A_train , b_train, scoring = \"neg_mean_squared_error\", cv = 10)\n",
    "rmse_scores = np.sqrt(-scores) \n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard Deviation:\", scores.std())\n",
    "    \n",
    "display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98667363470081\n",
      "0.9848484848484849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "bag_oob_clf = BaggingClassifier( DecisionTreeClassifier(), n_estimators = 10, max_samples = 50, bootstrap = True, n_jobs = -1, oob_score = True)\n",
    "bag_oob_clf.fit(A_train, b_train)\n",
    "\n",
    "b_pred = bag_oob_clf.predict(A_test)\n",
    "print(bag_oob_clf.oob_score_)\n",
    "print(accuracy_score(b_test, b_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8510971786833855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 1), n_estimators = 100, algorithm = \"SAMME.R\" , learning_rate = 0.2)\n",
    "ada_clf.fit(A_train, b_train)\n",
    "b_pred = ada_clf.predict(A_test)\n",
    "print(accuracy_score(b_test, b_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 2), n_estimators = 200, algorithm = \"SAMME.R\", learning_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "          learning_rate=0.5, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.fit(A_train, b_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=1.0, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2, n_estimators = 100, learning_rate = 1.0)\n",
    "gbrt.fit(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt2 = GradientBoostingRegressor(max_depth = 2, n_estimators = 100, learning_rate = 0.1)\n",
    "gbrt2.fit(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2650215283382731, 0.17931632724566746, 0.1232002474873442, 0.07921981887395156, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29, 4.6820584552302476e-29]\n"
     ]
    }
   ],
   "source": [
    "error = [mean_squared_error(b_test, b_pred) for b_pred in gbrt.staged_predict(A_test)]\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "best_n_estimator = np.argmin(error) # argmin : return indices f minimum value along the array\n",
    "print(best_n_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=4, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "gbrt2 = GradientBoostingRegressor(max_depth = 2, n_estimators = best_n_estimator)\n",
    "gbrt2.fit(A_train, b_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1 = gbrt.predict(A_test)\n",
    "acc2 = gbrt2.predict(A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without staged predict 1.0\n",
      "With staged predict 0.24033437826541273\n"
     ]
    }
   ],
   "source": [
    "print(\"Without staged predict\", accuracy_score(b_test, acc1.round()))\n",
    "print(\"With staged predict\", accuracy_score(b_test, acc2.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ext_clf = ExtraTreesClassifier()\n",
    "ext_clf.fit(A_train, b_train)\n",
    "b_pred = ext_clf.predict(A_test)\n",
    "print(accuracy_score(b_test, b_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_clf = SVC(kernel = \"poly\", degree = 3, coef0 = 1, C = 5)\n",
    "svm_clf.fit(A_train , b_train)\n",
    "b_pred = svm_clf.predict(A_test)\n",
    "print(accuracy_score(b_test, b_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
